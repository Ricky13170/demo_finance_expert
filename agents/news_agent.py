import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict
import logging
import textwrap
from tools.rag_chroma import RAGChroma

class NewsAgent:
    """Agent t√¨m ki·∫øm v√† t√≥m t·∫Øt tin t·ª©c t√†i ch√≠nh t·ª´ nhi·ªÅu ngu·ªìn (Cafef, VnExpress, VietStock)."""

    def __init__(self, max_articles_per_source: int = 3):
        self.rag = RAGChroma(collection_name="finance_news")
        self.headers = {'User-Agent': 'Mozilla/5.0'}
        self.max_articles_per_source = max_articles_per_source
        self.results = {}
        self.last_query = None

    # === Entry point ch√≠nh ===
    def run(self, symbol: str) -> Dict:
        """T√¨m tin t·ª©c v√† t·∫°o t√≥m t·∫Øt cho m√£ c·ªï phi·∫øu"""
        self.last_query = symbol.upper()
        all_articles = []
        all_articles.extend(self.search_cafef(symbol))
        # all_articles.extend(self.search_vnexpress(symbol))
        # all_articles.extend(self.search_vietstock(symbol))

        for a in all_articles:
            a["sentiment"] = self.get_sentiment_from_title(a["title"])

        summary_text = self.create_summary(symbol, all_articles)
        self.results = {
            "symbol": symbol,
            "timestamp": datetime.now().isoformat(),
            "articles": all_articles,
            "summary": summary_text
        }
        return self.results

    # === Crawl tin t·ª´ CafeF ===
    def search_cafef(self, symbol: str) -> List[Dict]:
        try:
            url = f"https://cafef.vn/tim-kiem.chn?keywords={symbol}"
            soup = BeautifulSoup(requests.get(url, headers=self.headers, timeout=10).content, 'html.parser')
            items = soup.find_all('div', class_='item', limit=self.max_articles_per_source)
            results = []
            for item in items:
                title_tag = item.find('h3')
                link_tag = item.find('a')
                desc_tag = item.find('p')
                if title_tag and link_tag:
                    results.append({
                        "source": "CafeF",
                        "title": title_tag.get_text(strip=True),
                        "url": 'https://cafef.vn' + link_tag['href'] if link_tag['href'].startswith('/') else link_tag['href'],
                        "description": desc_tag.get_text(strip=True) if desc_tag else '',
                        "date": datetime.now().strftime('%Y-%m-%d')
                    })
            return results
        except Exception as e:
            logging.warning(f"[CafeF] L·ªói crawl: {e}")
            return []

    # === Ph√¢n t√≠ch c·∫£m x√∫c ƒë∆°n gi·∫£n ===
    def get_sentiment_from_title(self, title: str) -> str:
        pos = ['tƒÉng', 'l·ª£i nhu·∫≠n', 'kh·∫£ quan', 'v∆∞·ª£t', 'k·ª∑ l·ª•c']
        neg = ['gi·∫£m', 'l·ªó', 's·ª•t', 'r·ªßi ro', 'th·∫•t b·∫°i']
        t = title.lower()
        pos_count = sum(w in t for w in pos)
        neg_count = sum(w in t for w in neg)
        return 'positive' if pos_count > neg_count else 'negative' if neg_count > pos_count else 'neutral'

    # === Sinh t√≥m t·∫Øt d√†i (200‚Äì300 ch·ªØ) ===
    def create_summary(self, symbol: str, articles: List[Dict]) -> str:
        if not articles:
            return f"Kh√¥ng t√¨m th·∫•y tin t·ª©c m·ªõi v·ªÅ {symbol.upper()} g·∫ßn ƒë√¢y."

        pos = sum(a['sentiment'] == 'positive' for a in articles)
        neg = sum(a['sentiment'] == 'negative' for a in articles)
        overview = "üìà Xu h∆∞·ªõng t√≠ch c·ª±c" if pos > neg else "üìâ Xu h∆∞·ªõng ti√™u c·ª±c" if neg > pos else "‚û°Ô∏è Trung t√≠nh"

        body = ""
        for i, a in enumerate(articles, 1):
            emoji = {'positive':'‚úÖ','negative':'‚ùå','neutral':'‚ÑπÔ∏è'}[a['sentiment']]
            body += f"{i}. {emoji} {a['title']} ({a['source']})\n"
            if a['description']:
                body += f"   {a['description']}\n"
            body += f"   üîó {a['url']}\n\n"

        full_text = f"üì∞ Tin t·ª©c m·ªõi nh·∫•t v·ªÅ {symbol.upper()} ({overview})\n\n{body.strip()}"
        return textwrap.shorten(full_text, width=300, placeholder="...")

    # === M·ªü r·ªông th√¥ng tin khi ng∆∞·ªùi d√πng h·ªèi ti·∫øp ===
    def expand_summary(self, article_index: int = None) -> str:
        """Cung c·∫•p th√™m chi ti·∫øt v·ªÅ tin t·ª©c tr∆∞·ªõc ƒë√≥"""
        if not self.results or "articles" not in self.results:
            return "‚ö†Ô∏è Hi·ªán ch∆∞a c√≥ d·ªØ li·ªáu tin t·ª©c tr∆∞·ªõc ƒë√≥. H√£y h·ªèi t√¥i v·ªÅ m·ªôt m√£ c·ªï phi·∫øu tr∆∞·ªõc nh√©!"

        articles = self.results["articles"]
        if article_index is not None and 1 <= article_index <= len(articles):
            a = articles[article_index - 1]
            detail = f"üì∞ Chi ti·∫øt tin #{article_index}: {a['title']}\nüìÖ Ng√†y: {a['date']}\nüîó {a['url']}\n"
            detail += f"M√¥ t·∫£: {a['description'] or 'Kh√¥ng c√≥ m√¥ t·∫£ chi ti·∫øt.'}"
            return detail
        else:
            # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh tin c·ª• th·ªÉ ‚Üí m·ªü r·ªông to√†n b·ªô danh s√°ch
            full_list = "\n".join([
                f"{i+1}. {a['title']} ({a['source']}) - {a['url']}"
                for i, a in enumerate(articles)
            ])
            return f"üìñ ƒê√¢y l√† danh s√°ch ƒë·∫ßy ƒë·ªß c√°c tin ƒë√£ thu th·∫≠p:\n\n{full_list}"
