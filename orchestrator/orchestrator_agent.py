# orchestrator_agent.py (phi√™n b·∫£n Groq-compatible)

import os
import json
import asyncio
from dotenv import load_dotenv
from typing import Optional
from openai import AsyncOpenAI

from orchestrator.prompt_templates import ORCHESTRATOR_PROMPT
from agents.stock_agent import StockAgent
from agents.advice_agent import analyze_stock
from rag.rag_engine import RagEngine
from memory.conversation_memory import ConversationMemory


class OrchestratorAgent:
    def __init__(
        self,
        model_name: str = "meta-llama/llama-4-scout-17b-16e-instruct",
        rag_engine: Optional[RagEngine] = None,
        memory: Optional[ConversationMemory] = None,
    ):
        load_dotenv()
        self.model_name = model_name
        self.api_key = os.getenv("GROQ_API_KEY")
        self.stock_agent = StockAgent()
        self.rag_engine = rag_engine or RagEngine()
        self.memory = memory or ConversationMemory()

        if not self.api_key:
            raise ValueError("‚ùå Thi·∫øu GROQ_API_KEY trong .env")

        # Groq client theo chu·∫©n OpenAI
        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url="https://api.groq.com/openai/v1"
        )

    async def _call_llm(self, prompt: str) -> str:
        """G·ªçi Groq API (OpenAI-compatible async call)"""
        try:
            resp = await self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.6,
                max_tokens=500,
            )
            return resp.choices[0].message.content.strip()
        except Exception as e:
            print("[WARN] Groq LLM call failed:", e)
            return ""

    async def handle_query(self, query: str, user_id: str = "default") -> str:
        # --- 1Ô∏è‚É£ L∆∞u l·ªãch s·ª≠ h·ªôi tho·∫°i
        self.memory.add_message(user_id, "user", query)

        # --- 2Ô∏è‚É£ ƒê·ªãnh tuy·∫øn (intent routing)
        routing_prompt = ORCHESTRATOR_PROMPT.format(query=query)
        try:
            routing_text = await self._call_llm(routing_prompt)
        except Exception:
            routing_text = ""

        rd = (routing_text or "").lower()
        if any(k in rd for k in ["advice", "t∆∞ v·∫•n", "khuy·∫øn ngh·ªã"]):
            intent = "advice_query"
        elif any(k in rd for k in ["price", "gi√°", "price_query"]):
            intent = "price_query"
        else:
            lower = query.lower()
            if any(k in lower for k in ["gi√°", "h√¥m nay", "h√¥m qua", "bao nhi√™u", "tƒÉng", "gi·∫£m"]):
                intent = "price_query"
            elif any(k in lower for k in ["c√≥ n√™n", "mua", "b√°n", "ph√¢n t√≠ch", "ƒë·∫ßu t∆∞", "d·ª± ƒëo√°n"]):
                intent = "advice_query"
            else:
                intent = "chat"

        # --- 3Ô∏è‚É£ Truy xu·∫•t context t·ª´ RAG (placeholder)
        try:
            context_text = self.rag_engine.retrieve_context(query)
            if context_text:
                print(f"üìö RAG context len: {len(context_text)} chars")
        except Exception as e:
            print("[WARN] RAG retrieval failed:", e)
            context_text = ""

        # --- 4Ô∏è‚É£ G·ªçi agent t∆∞∆°ng ·ª©ng
        if intent == "price_query":
            response = self.stock_agent.handle_request(query)
        elif intent == "advice_query":
            response = analyze_stock(query)
        else:
            response = "ü§ñ Xin ch√†o! M√¨nh l√† tr·ª£ l√Ω t√†i ch√≠nh. B·∫°n c√≥ th·ªÉ h·ªèi v·ªÅ gi√° c·ªï phi·∫øu ho·∫∑c t∆∞ v·∫•n."

        # --- 5Ô∏è‚É£ Chu·∫©n b·ªã prompt ƒë·ªÉ LLM t·ªïng h·ª£p l·∫°i
        recent = self.memory.get_recent(user_id, n=6)
        recent_text = "\n".join([f"{m['role']}: {m['text']}" for m in recent])

        final_prompt = f"""
Ng·ªØ c·∫£nh RAG (n·∫øu c√≥):
{context_text}

L·ªãch s·ª≠ h·ªôi tho·∫°i (g·∫ßn nh·∫•t):
{recent_text}

Ph·∫£n h·ªìi h·ªá th·ªëng:
{response}

H√£y di·ªÖn gi·∫£i l·∫°i b·∫±ng ti·∫øng Vi·ªát, ng·∫Øn g·ªçn, th√¢n thi·ªán, d√πng emoji n·∫øu ph√π h·ª£p.
"""

        try:
            final_text = await self._call_llm(final_prompt)
        except Exception as e:
            print("[WARN] LLM formatting failed:", e)
            final_text = ""

        if not final_text:
            final_text = response if isinstance(response, str) else str(response)

        # --- 6Ô∏è‚É£ L∆∞u ph·∫£n h·ªìi v√†o memory
        self.memory.add_message(user_id, "assistant", final_text)
        return final_text
